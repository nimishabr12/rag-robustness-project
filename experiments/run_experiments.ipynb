{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Robustness Experiments - Analysis & Visualization\n",
    "\n",
    "This notebook loads experiment results and provides comprehensive visualizations of RAG system performance across different noise types and retrieval strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Make plots larger\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "results_path = '../results/experiment_results.json'\n",
    "\n",
    "with open(results_path, 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(f\"Loaded results from: {results_path}\")\n",
    "print(f\"Timestamp: {results['metadata']['timestamp']}\")\n",
    "print(f\"\\nNoise types: {results['metadata']['noise_types']}\")\n",
    "print(f\"Retrieval strategies: {results['metadata']['retrieval_strategies']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = results['summary']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"OVERALL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal Queries: {summary['overall']['total_queries']}\")\n",
    "print(f\"Average Precision@5: {summary['overall']['avg_precision_at_5']:.3f}\")\n",
    "print(f\"Standard Deviation: {summary['overall']['std_precision_at_5']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Heatmap: Performance Across Noise Types and Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for heatmap\n",
    "noise_types = results['metadata']['noise_types']\n",
    "strategies = results['metadata']['retrieval_strategies']\n",
    "\n",
    "# Build matrix of average precision@5 scores\n",
    "heatmap_data = []\n",
    "\n",
    "for noise_type in noise_types:\n",
    "    row = []\n",
    "    for strategy in strategies:\n",
    "        if noise_type in summary['by_noise_type'] and strategy in summary['by_noise_type'][noise_type]['strategies']:\n",
    "            score = summary['by_noise_type'][noise_type]['strategies'][strategy]['avg_precision_at_5']\n",
    "            row.append(score)\n",
    "        else:\n",
    "            row.append(0.0)\n",
    "    heatmap_data.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "df_heatmap = pd.DataFrame(\n",
    "    heatmap_data,\n",
    "    index=[nt.replace('_', ' ').title() for nt in noise_types],\n",
    "    columns=[s.replace('_retrieval', '').replace('_', ' ').title() for s in strategies]\n",
    ")\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(\n",
    "    df_heatmap,\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    cmap='RdYlGn',\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cbar_kws={'label': 'Precision@5'},\n",
    "    linewidths=0.5\n",
    ")\n",
    "plt.title('RAG Performance Heatmap: Precision@5 by Noise Type and Strategy', fontsize=16, pad=20)\n",
    "plt.xlabel('Retrieval Strategy', fontsize=13)\n",
    "plt.ylabel('Noise Type', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/heatmap_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Heatmap saved to: results/heatmap_performance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bar Chart: Strategy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract strategy performance data\n",
    "strategy_names = []\n",
    "strategy_scores = []\n",
    "strategy_stds = []\n",
    "\n",
    "for strategy in strategies:\n",
    "    if strategy in summary['by_strategy']:\n",
    "        strategy_names.append(strategy.replace('_retrieval', '').replace('_', ' ').title())\n",
    "        strategy_scores.append(summary['by_strategy'][strategy]['avg_precision_at_5'])\n",
    "        strategy_stds.append(summary['by_strategy'][strategy]['std_precision_at_5'])\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "x_pos = np.arange(len(strategy_names))\n",
    "bars = ax.bar(x_pos, strategy_scores, yerr=strategy_stds, capsize=5, alpha=0.8, edgecolor='black')\n",
    "\n",
    "# Color bars based on performance\n",
    "colors = plt.cm.RdYlGn(np.array(strategy_scores))\n",
    "for bar, color in zip(bars, colors):\n",
    "    bar.set_color(color)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (score, std) in enumerate(zip(strategy_scores, strategy_stds)):\n",
    "    ax.text(i, score + std + 0.02, f'{score:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Retrieval Strategy', fontsize=13)\n",
    "ax.set_ylabel('Average Precision@5', fontsize=13)\n",
    "ax.set_title('Retrieval Strategy Performance Comparison\\n(averaged across all noise types)', fontsize=16, pad=20)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(strategy_names, rotation=15, ha='right')\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/strategy_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Bar chart saved to: results/strategy_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Grouped Bar Chart: Performance by Noise Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grouped bar chart\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "x = np.arange(len(noise_types))\n",
    "width = 0.2\n",
    "multiplier = 0\n",
    "\n",
    "for strategy in strategies:\n",
    "    scores = []\n",
    "    for noise_type in noise_types:\n",
    "        if noise_type in summary['by_noise_type'] and strategy in summary['by_noise_type'][noise_type]['strategies']:\n",
    "            score = summary['by_noise_type'][noise_type]['strategies'][strategy]['avg_precision_at_5']\n",
    "            scores.append(score)\n",
    "        else:\n",
    "            scores.append(0.0)\n",
    "    \n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, scores, width, label=strategy.replace('_retrieval', '').replace('_', ' ').title())\n",
    "    multiplier += 1\n",
    "\n",
    "ax.set_xlabel('Noise Type', fontsize=13)\n",
    "ax.set_ylabel('Precision@5', fontsize=13)\n",
    "ax.set_title('Strategy Performance Across Different Noise Types', fontsize=16, pad=20)\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels([nt.replace('_', ' ').title() for nt in noise_types], rotation=15, ha='right')\n",
    "ax.legend(loc='upper right', framealpha=0.9)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/noise_type_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Grouped bar chart saved to: results/noise_type_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detailed Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed statistics table\n",
    "stats_data = []\n",
    "\n",
    "for strategy in strategies:\n",
    "    if strategy in summary['by_strategy']:\n",
    "        stats = summary['by_strategy'][strategy]\n",
    "        stats_data.append({\n",
    "            'Strategy': strategy.replace('_retrieval', '').replace('_', ' ').title(),\n",
    "            'Queries': stats['num_queries'],\n",
    "            'Avg P@5': f\"{stats['avg_precision_at_5']:.3f}\",\n",
    "            'Std Dev': f\"{stats['std_precision_at_5']:.3f}\",\n",
    "            'Min': f\"{stats['min_precision_at_5']:.3f}\",\n",
    "            'Max': f\"{stats['max_precision_at_5']:.3f}\"\n",
    "        })\n",
    "\n",
    "df_stats = pd.DataFrame(stats_data)\n",
    "print(\"\\nDetailed Strategy Statistics:\")\n",
    "print(\"=\"*80)\n",
    "display(df_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Example Failures for Each Noise Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display example failures\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE FAILURES BY NOISE TYPE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for noise_type in noise_types:\n",
    "    if noise_type in results['examples'] and results['examples'][noise_type]:\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"NOISE TYPE: {noise_type.upper().replace('_', ' ')}\")\n",
    "        print(f\"{'─'*80}\\n\")\n",
    "        \n",
    "        for strategy, failures in results['examples'][noise_type].items():\n",
    "            if failures:\n",
    "                print(f\"\\nStrategy: {strategy.replace('_retrieval', '').replace('_', ' ').title()}\")\n",
    "                print(\"-\" * 80)\n",
    "                \n",
    "                for i, failure in enumerate(failures[:2], 1):  # Show top 2 failures per strategy\n",
    "                    print(f\"\\nExample {i}:\")\n",
    "                    print(f\"  Query ID: {failure['query_id']}\")\n",
    "                    print(f\"  Query: {failure['query']}\")\n",
    "                    if failure['query'] != failure['original_query']:\n",
    "                        print(f\"  Original: {failure['original_query']}\")\n",
    "                    print(f\"  Query Type: {failure['query_type']}\")\n",
    "                    \n",
    "                    precision = failure['evaluation']['precision'].get('p@5', {}).get('precision_at_k', 0.0)\n",
    "                    print(f\"  Precision@5: {precision:.3f}\")\n",
    "                    \n",
    "                    print(f\"  Generated Answer: {failure['generated_answer'][:200]}...\")\n",
    "                    \n",
    "                    if failure['retrieved_passages']:\n",
    "                        print(f\"  Top Retrieved Passage: {failure['retrieved_passages'][0]['text'][:150]}...\")\n",
    "                    \n",
    "                    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Degradation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance degradation from clean baseline\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE DEGRADATION FROM CLEAN BASELINE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "degradation_data = []\n",
    "\n",
    "for strategy in strategies:\n",
    "    strategy_name = strategy.replace('_retrieval', '').replace('_', ' ').title()\n",
    "    \n",
    "    # Get clean baseline\n",
    "    if 'clean' in summary['by_noise_type'] and strategy in summary['by_noise_type']['clean']['strategies']:\n",
    "        baseline = summary['by_noise_type']['clean']['strategies'][strategy]['avg_precision_at_5']\n",
    "        \n",
    "        print(f\"\\n{strategy_name}:\")\n",
    "        print(f\"  Baseline (clean): {baseline:.3f}\")\n",
    "        print(f\"  Performance with noise:\")\n",
    "        \n",
    "        for noise_type in ['noisy', 'ambiguous', 'context_dependent', 'adversarial']:\n",
    "            if noise_type in summary['by_noise_type'] and strategy in summary['by_noise_type'][noise_type]['strategies']:\n",
    "                score = summary['by_noise_type'][noise_type]['strategies'][strategy]['avg_precision_at_5']\n",
    "                degradation = ((baseline - score) / baseline * 100) if baseline > 0 else 0\n",
    "                \n",
    "                print(f\"    {noise_type.replace('_', ' ').title():20s}: {score:.3f} (-{degradation:.1f}%)\")\n",
    "                \n",
    "                degradation_data.append({\n",
    "                    'Strategy': strategy_name,\n",
    "                    'Noise Type': noise_type.replace('_', ' ').title(),\n",
    "                    'Degradation (%)': degradation\n",
    "                })\n",
    "\n",
    "# Visualize degradation\n",
    "if degradation_data:\n",
    "    df_degradation = pd.DataFrame(degradation_data)\n",
    "    df_pivot = df_degradation.pivot(index='Noise Type', columns='Strategy', values='Degradation (%)')\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    df_pivot.plot(kind='bar', ax=plt.gca(), width=0.8)\n",
    "    plt.title('Performance Degradation from Clean Baseline', fontsize=16, pad=20)\n",
    "    plt.xlabel('Noise Type', fontsize=13)\n",
    "    plt.ylabel('Degradation (%)', fontsize=13)\n",
    "    plt.legend(title='Strategy', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xticks(rotation=15, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/performance_degradation.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nDegradation chart saved to: results/performance_degradation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Query Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by query type\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE BY QUERY TYPE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "query_type_stats = {}\n",
    "\n",
    "for noise_type, strategies_dict in results['detailed_results'].items():\n",
    "    for strategy, result_list in strategies_dict.items():\n",
    "        for result in result_list:\n",
    "            q_type = result.get('query_type', 'unknown')\n",
    "            \n",
    "            if q_type not in query_type_stats:\n",
    "                query_type_stats[q_type] = []\n",
    "            \n",
    "            precision = result['evaluation']['precision'].get('p@5', {}).get('precision_at_k', 0.0)\n",
    "            query_type_stats[q_type].append(precision)\n",
    "\n",
    "# Calculate averages\n",
    "query_type_avgs = {}\n",
    "for q_type, precisions in query_type_stats.items():\n",
    "    if precisions:\n",
    "        query_type_avgs[q_type] = {\n",
    "            'count': len(precisions),\n",
    "            'avg': np.mean(precisions),\n",
    "            'std': np.std(precisions)\n",
    "        }\n",
    "\n",
    "# Sort by average precision\n",
    "sorted_types = sorted(query_type_avgs.items(), key=lambda x: x[1]['avg'], reverse=True)\n",
    "\n",
    "print(f\"{'Query Type':<20} {'Count':<10} {'Avg P@5':<12} {'Std Dev':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for q_type, stats in sorted_types:\n",
    "    print(f\"{q_type:<20} {stats['count']:<10} {stats['avg']:<12.3f} {stats['std']:<10.3f}\")\n",
    "\n",
    "# Visualize\n",
    "if query_type_avgs:\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    types = [t[0] for t in sorted_types]\n",
    "    avgs = [t[1]['avg'] for t in sorted_types]\n",
    "    stds = [t[1]['std'] for t in sorted_types]\n",
    "    \n",
    "    bars = ax.bar(range(len(types)), avgs, yerr=stds, capsize=5, alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    # Color bars\n",
    "    colors = plt.cm.RdYlGn(np.array(avgs))\n",
    "    for bar, color in zip(bars, colors):\n",
    "        bar.set_color(color)\n",
    "    \n",
    "    ax.set_xlabel('Query Type', fontsize=13)\n",
    "    ax.set_ylabel('Average Precision@5', fontsize=13)\n",
    "    ax.set_title('Performance by Query Type\\n(averaged across all strategies and noise types)', fontsize=16, pad=20)\n",
    "    ax.set_xticks(range(len(types)))\n",
    "    ax.set_xticklabels(types, rotation=45, ha='right')\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/query_type_performance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nQuery type chart saved to: results/query_type_performance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a comprehensive analysis of RAG system robustness across different noise types and retrieval strategies. Key visualizations include:\n",
    "\n",
    "1. **Heatmap** - Overall performance matrix\n",
    "2. **Bar Charts** - Strategy and noise type comparisons\n",
    "3. **Example Failures** - Specific cases where the system struggled\n",
    "4. **Degradation Analysis** - How performance degrades with different noise types\n",
    "5. **Query Type Analysis** - Performance across different question types\n",
    "\n",
    "All visualizations are saved to the `results/` directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
